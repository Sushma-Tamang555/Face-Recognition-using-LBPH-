{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa8c9de",
   "metadata": {},
   "source": [
    "# Generating data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e36d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce7a45a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12880\\2845809390.py:7: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if faces is ():\n"
     ]
    }
   ],
   "source": [
    "def generate_dataset():\n",
    "    face_classifier=cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    def face_cropped(img):\n",
    "        gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "        faces=face_classifier.detectMultiScale(gray,1.3,5)\n",
    "        \n",
    "        if faces is ():\n",
    "            return None\n",
    "        for (x,y,w,h) in faces:\n",
    "            cropped_face=img[y:y+h,x:x+w]\n",
    "        return cropped_face\n",
    "            \n",
    "    cap=cv2.VideoCapture(0)\n",
    "    img_id=60\n",
    "    \n",
    "    \n",
    "    while True:\n",
    "        ret, frame=cap.read()\n",
    "        if face_cropped(frame) is not None:\n",
    "            img_id +=1\n",
    "            face=cv2.resize(face_cropped(frame),(200,200))\n",
    "            face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "            #file_name_path=\"data/\"+\"Santosh.\"+str(img_id)+\".jpg\"\n",
    "            file_name_path=\"Image for visualization/\"+str(img_id)+\".jpg\"\n",
    "            cv2.imwrite(file_name_path,face)\n",
    "            cv2.putText(face,str(img_id),(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)\n",
    "            \n",
    "            \n",
    "            cv2.imshow(\"Cropped_Face\", face)\n",
    "            if cv2.waitKey(1)==13 or int(img_id)==80:#13 is the ASCI charater of enter jasle chahi infinite loopma cameralai lana baat rokxa\n",
    "                break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    print(\"Collecting samples is completed\")\n",
    "                \n",
    "#generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aec2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189afd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_label(image_name):\n",
    "    name=image_name.split('.')[-3]\n",
    "    #if i have two person in my dataset\n",
    "    #if name==\"Sushma\":\n",
    "        #return np.array([1,0])\n",
    "    #elif name==\"Shreeyukta\":\n",
    "       # return np.array([0,1])\n",
    "    \n",
    "    #if i have three persons in my dataset\n",
    "    if name==\"Sushma\":\n",
    "        return np.array([1,0,0,0])\n",
    "    elif name==\"Shreeyukta\":\n",
    "        return np.array([0,1,0,0])\n",
    "    elif name==\"Shirjana\":\n",
    "        return np.array([0,0,1,0])\n",
    "    elif name==\"Santosh\":\n",
    "        return np.array([0,0,0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc006daa",
   "metadata": {},
   "source": [
    "#Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43acb5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d284a2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_data():\n",
    "    data=[]\n",
    "    for img in tqdm(os.listdir(\"data\")):\n",
    "        path=os.path.join(\"data\",img)\n",
    "        img_data=cv2.imread(path,cv2.IMREAD_GRAYSCALE)\n",
    "        img_data=cv2.resize(img_data,(50,50))\n",
    "        data.append([np.array(img_data),my_label(img)])\n",
    "    shuffle(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20ded8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 4000/4000 [00:02<00:00, 1548.55it/s]\n"
     ]
    }
   ],
   "source": [
    "data=my_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831a8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3400, 50, 50, 1)\n",
      "(600, 50, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "train=data[:3400] \n",
    "test=data[3400:]\n",
    "X_train=np.array([i[0] for i in train]).reshape(-1,50,50,1)\n",
    "print(X_train.shape)\n",
    "y_train =[i[1] for i in train]\n",
    "X_test=np.array([i[0] for i in test]).reshape(-1,50,50,1)\n",
    "print(X_test.shape)\n",
    "y_test=[i[1] for i in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1374f",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84942de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\user\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "#import warnings\n",
    "#warnings.filterwarning('ignore')\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8310a51d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: FRS\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 3400\n",
      "Validation samples: 600\n",
      "--\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 4) for Tensor TargetsData/Y:0, which has shape (?, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12880\\2942836419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mconvnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconvnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard_verbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'FRS'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;31m# Retrieve data preprocesing and augmentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[0mdaug_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdprep_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve_data_preprocessing_and_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m         self.trainer.fit(feed_dicts, val_feed_dicts=val_feed_dicts,\n\u001b[0m\u001b[0;32m    197\u001b[0m                          \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                          \u001b[0mshow_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_metric\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[0;32m    339\u001b[0m                             \u001b[0mcaller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_sub_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m                             snapshot = train_op._train(self.training_state.step,\n\u001b[0m\u001b[0;32m    342\u001b[0m                                                        \u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[0mfeed_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0m\u001b[0;32m    828\u001b[0m                                              feed_batch)\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1163\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m   1164\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[1;32m-> 1165\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1166\u001b[0m                 \u001b[1;34mf'Cannot feed value of shape {str(np_val.shape)} for Tensor '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m                 \u001b[1;34mf'{subfeed_t.name}, which has shape '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64, 4) for Tensor TargetsData/Y:0, which has shape (?, 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "tf.reset_default_graph()\n",
    "convnet= input_data(shape=[50,50,1])\n",
    "convnet=conv_2d(convnet, 32, 5, activation='relu')\n",
    "#32 filters and stride=5 so that the filter will move 5 pixel or unit at time\n",
    "convnet=max_pool_2d(convnet, 5)\n",
    "convnet=conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet=max_pool_2d(convnet, 5)\n",
    "convnet=conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet=max_pool_2d(convnet, 5)\n",
    "convnet=conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet=max_pool_2d(convnet, 5)\n",
    "convnet=conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet=max_pool_2d(convnet, 5)\n",
    "\n",
    "\n",
    "\n",
    "convnet=fully_connected(convnet, 1024, activation='relu')\n",
    "convnet=dropout(convnet, 0.8)\n",
    "convnet=fully_connected(convnet, 4, activation='softmax') # output layer\n",
    "convnet=regression(convnet, optimizer='adam', learning_rate=0.001, loss='categorical_crossentropy')\n",
    "model=tflearn.DNN(convnet, tensorboard_verbose=1)\n",
    "model.fit(X_train, y_train, n_epoch=50, validation_set=(X_test, y_test), show_metric=True, run_id='FRS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3d91cd",
   "metadata": {},
   "source": [
    "# train the classifier or prediction the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7addfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79cd593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def data_for_visualization():\n",
    "    Vdata=[]\n",
    "    for img in tqdm(os.listdir(\"Image for visualization\")):\n",
    "        path=os.path.join(\"Image for visualization\", img)\n",
    "        img_num=img.split('.')[0]\n",
    "        img_data=cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "        img_data=cv2.resize(img_data, (50,50))\n",
    "        Vdata.append([np.array(img_data), img_num])\n",
    "    shuffle(Vdata)\n",
    "    return Vdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b65a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vdata=data_for_visualization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d28c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure(figsize=(20,20))\n",
    "for num, data in enumerate(Vdata[:20]):\n",
    "    img_data=data[0]\n",
    "    y=fig.add_subplot(5,5,num+1)\n",
    "    image=img_data\n",
    "    data=img_data.reshape(50,50,1)\n",
    "    model_out=model.predict([data])[0]\n",
    "    \n",
    "    if np.argmax(model_out)== 0:\n",
    "        my_label=\"Sushma\"\n",
    "    elif np.argmax(model_out)== 1:\n",
    "        my_label=\"Shreeyukta\"\n",
    "    elif np.argmax(model_out)== 0:\n",
    "        my_label=\"Santosh\"\n",
    "    else:\n",
    "        my_label=\"Shirjana\"\n",
    "    \n",
    "    y.imshow(image, cmap='gray')\n",
    "    plt.title(my_label)\n",
    "    \n",
    "    y.axes.get_xaxis().set_visible(False)\n",
    "    y.axes.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3351348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
